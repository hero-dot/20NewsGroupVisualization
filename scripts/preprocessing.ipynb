{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~/Documents/20NewsGroupVisualization/preprocessing/data ~/Documents/20NewsGroupVisualization/preprocessing\n",
      "The data has already been downloaded...\n",
      "Lets take a look at the groups...\n",
      "alt.atheism\n",
      "comp.graphics\n",
      "comp.os.ms-windows.misc\n",
      "comp.sys.ibm.pc.hardware\n",
      "comp.sys.mac.hardware\n",
      "comp.windows.x\n",
      "misc.forsale\n",
      "rec.autos\n",
      "rec.motorcycles\n",
      "rec.sport.baseball\n",
      "rec.sport.hockey\n",
      "sci.crypt\n",
      "sci.electronics\n",
      "sci.med\n",
      "sci.space\n",
      "soc.religion.christian\n",
      "talk.politics.guns\n",
      "talk.politics.mideast\n",
      "talk.politics.misc\n",
      "talk.religion.misc\n",
      "~/Documents/20NewsGroupVisualization/preprocessing\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "mkdir -p data\n",
    "pushd data\n",
    "if [ -d \"20news-bydate-train\" ]\n",
    "then\n",
    "  echo \"The data has already been downloaded...\"\n",
    "else\n",
    "  wget http://qwone.com/%7Ejason/20Newsgroups/20news-bydate.tar.gz\n",
    "  tar xfv 20news-bydate.tar.gz\n",
    "  rm 20news-bydate.tar.gz\n",
    "fi\n",
    "echo \"Lets take a look at the groups...\"\n",
    "ls 20news-bydate-train/\n",
    "popd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 peter peter 1.5K Mar 18  2003 61250\r\n",
      "-rw-r--r--  1 peter peter  889 Mar 18  2003 61252\r\n",
      "-rw-r--r--  1 peter peter 1.2K Mar 18  2003 61264\r\n",
      "-rw-r--r--  1 peter peter 1.7K Mar 18  2003 61308\r\n",
      "-rw-r--r--  1 peter peter 1.4K Mar 18  2003 61422\r\n"
     ]
    }
   ],
   "source": [
    "ls -lah data/20news-bydate-train/sci.space | tail -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: ralph.buttigieg@f635.n713.z3.fido.zeta.org.au (Ralph Buttigieg)\r\n",
      "Subject: Why not give $1 billion to first year-lo\r\n",
      "Organization: Fidonet. Gate admin is fido@socs.uts.edu.au\r\n",
      "Lines: 34\r\n",
      "\r\n",
      "Original to: keithley@apple.com\r\n",
      "G'day keithley@apple.com\r\n",
      "\r\n",
      "21 Apr 93 22:25, keithley@apple.com wrote to All:\r\n",
      "\r\n",
      " kc> keithley@apple.com (Craig Keithley), via Kralizec 3:713/602\r\n",
      "\r\n",
      "\r\n",
      " kc> But back to the contest goals, there was a recent article in AW&ST\r\n",
      "about a\r\n",
      " kc> low cost (it's all relative...) manned return to the moon.  A General\r\n",
      " kc> Dynamics scheme involving a Titan IV & Shuttle to lift a Centaur upper\r\n",
      " kc> stage, LEV, and crew capsule.  The mission consists of delivering two\r\n",
      " kc> unmanned payloads to the lunar surface, followed by a manned mission.\r\n",
      " kc> Total cost:  US was $10-$13 billion.  Joint ESA(?)/NASA project was\r\n"
     ]
    }
   ],
   "source": [
    "!head data/20news-bydate-train/sci.space/61422 -n 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and tokenizing the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import re\n",
    "import string\n",
    "import funcy as fp\n",
    "from gensim import models\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "import nltk\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group</th>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">rec.sport.baseball</th>\n",
       "      <th>104363</th>\n",
       "      <td>[From: re4@prism.gatech.EDU (RUSSELL EARNEST)\\...</td>\n",
       "      <td>[from, #email, russell, earnest, subject, play...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102652</th>\n",
       "      <td>[From: niepornt@phoenix.Princeton.EDU (David M...</td>\n",
       "      <td>[from, #email, david, marc, nieporent, subject...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104503</th>\n",
       "      <td>[From: stlouis@unixg.ubc.ca (Phill St. Louis)\\...</td>\n",
       "      <td>[from, #email, phill, louis, subject, billy, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104411</th>\n",
       "      <td>[From: krattige@hpcc01.corp.hp.com (Kim Kratti...</td>\n",
       "      <td>[from, #email, kim, krattiger, subject, kevin,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102659</th>\n",
       "      <td>[From: carrd@iccgcc.decnet.ab.com\\n, Subject: ...</td>\n",
       "      <td>[from, #email, subject, david, wells, lines, h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                         doc  \\\n",
       "group              id                                                          \n",
       "rec.sport.baseball 104363  [From: re4@prism.gatech.EDU (RUSSELL EARNEST)\\...   \n",
       "                   102652  [From: niepornt@phoenix.Princeton.EDU (David M...   \n",
       "                   104503  [From: stlouis@unixg.ubc.ca (Phill St. Louis)\\...   \n",
       "                   104411  [From: krattige@hpcc01.corp.hp.com (Kim Kratti...   \n",
       "                   102659  [From: carrd@iccgcc.decnet.ab.com\\n, Subject: ...   \n",
       "\n",
       "                                                                      tokens  \n",
       "group              id                                                         \n",
       "rec.sport.baseball 104363  [from, #email, russell, earnest, subject, play...  \n",
       "                   102652  [from, #email, david, marc, nieporent, subject...  \n",
       "                   104503  [from, #email, phill, louis, subject, billy, t...  \n",
       "                   104411  [from, #email, kim, krattiger, subject, kevin,...  \n",
       "                   102659  [from, #email, subject, david, wells, lines, h...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick and dirty....\n",
    "EMAIL_REGEX = re.compile(r\"[a-z0-9\\.\\+_-]+@[a-z0-9\\._-]+\\.[a-z]*\")\n",
    "FILTER_REGEX = re.compile(r\"[^a-z '#]\")\n",
    "TOKEN_MAPPINGS = [(EMAIL_REGEX, \"#email\"), (FILTER_REGEX, ' ')]\n",
    "\n",
    "def tokenize_line(line):\n",
    "    res = line.lower()\n",
    "    for regexp, replacement in TOKEN_MAPPINGS:\n",
    "        res = regexp.sub(replacement, res)\n",
    "    return res.split()\n",
    "    \n",
    "def tokenize(lines, token_size_filter=2):\n",
    "    tokens = fp.mapcat(tokenize_line, lines)\n",
    "    return [t for t in tokens if len(t) > token_size_filter]\n",
    "    \n",
    "\n",
    "def load_doc(filename):\n",
    "    group, doc_id = filename.split('/')[-2:]\n",
    "    with open(filename) as f:\n",
    "        doc = f.readlines()\n",
    "    return {'group': group,\n",
    "            'doc': doc,\n",
    "            'tokens': tokenize(doc),\n",
    "            'id': doc_id}\n",
    "\n",
    "\n",
    "docs = pd.DataFrame(list(map(load_doc, glob('data/20news-bydate-train/*/*')))).set_index(['group','id'])\n",
    "docs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the dictionary, and bag of words corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nltk_stopwords():\n",
    "    return set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def prep_corpus(docs, additional_stopwords=set(), no_below=5, no_above=0.5):\n",
    "  print('Building dictionary...')\n",
    "  dictionary = Dictionary(docs)\n",
    "  stopwords = nltk_stopwords().union(additional_stopwords)\n",
    "  stopword_ids = map(dictionary.token2id.get, stopwords)\n",
    "  dictionary.filter_tokens(stopword_ids)\n",
    "  dictionary.compactify()\n",
    "  dictionary.filter_extremes(no_below=no_below, no_above=no_above, keep_n=None)\n",
    "  dictionary.compactify()\n",
    "\n",
    "  print('Building corpus...')\n",
    "  corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "\n",
    "  return dictionary, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dictionary...\n",
      "Building corpus...\n"
     ]
    }
   ],
   "source": [
    "dictionary, corpus = prep_corpus(docs['tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the dictionary and the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MmCorpus.serialize('newsgroups.mm', corpus)\n",
    "dictionary.save('newgroups.dict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting the LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 12min 8s, sys: 2min 10s, total: 1h 14min 18s\n",
      "Wall time: 15min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lda = models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=50, passes=10)\n",
    "lda.save('newsgroups_50.model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
