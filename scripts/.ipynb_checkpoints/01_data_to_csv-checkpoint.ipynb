{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p data\n",
    "pushd data\n",
    "if [ -d \"20news-bydate-train\" ]\n",
    "then\n",
    "  echo \"The data has already been downloaded...\"\n",
    "else\n",
    "  wget http://qwone.com/%7Ejason/20Newsgroups/20news-bydate.tar.gz\n",
    "  tar xfv 20news-bydate.tar.gz\n",
    "  rm 20news-bydate.tar.gz\n",
    "fi\n",
    "echo \"Lets take a look at the groups...\"\n",
    "ls 20news-bydate-train/\n",
    "popd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ls -lah data/20news-bydate-train/sci.space | tail -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!head data/20news-bydate-train/sci.space/61422 -n 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and tokenizing the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import re\n",
    "import string\n",
    "import funcy as fp\n",
    "from gensim import models\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from pattern.en import parse\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(filename=\"logging.txt\", format='%(asctime)s : %(levelname)s : %(message)s',filemode =\"w\", level=logging.INFO)\n",
    "# quick and dirty...\n",
    "EMAIL_REGEX = re.compile(r\"[a-z0-9\\.\\+_-]+@[a-z0-9\\._-]+\\.[a-z]*\")\n",
    "FILTER_REGEX = re.compile(r\"[^a-z '#]\")\n",
    "TOKEN_MAPPINGS = [(EMAIL_REGEX, \"#email\"), (FILTER_REGEX, ' ')]\n",
    "\n",
    "def tokenize_line(line):\n",
    "    res = line.lower()\n",
    "    for regexp, replacement in TOKEN_MAPPINGS:\n",
    "        res = regexp.sub(replacement, res)\n",
    "\n",
    "    sentence = parse(res,tokenize=True,tags=False, chunks=False, relations= False, lemmata=True).split()\n",
    "    \n",
    "    # initialize the Variables\n",
    "    allowed_tags = re.compile('(NN|VB|JJ|RB)')\n",
    "    stopwords = frozenset()\n",
    "    min_length = 2\n",
    "    max_length = 15\n",
    "    result = []\n",
    "    \n",
    "    # lemmatization of the words\n",
    "    try:\n",
    "        sentence = sentence[0]\n",
    "    except IndexError:\n",
    "        pass \n",
    "    \n",
    "    for token, tag, lemma in sentence:\n",
    "        if min_length <= len(lemma) <= max_length and lemma not in stopwords:\n",
    "            if allowed_tags.match(tag):\n",
    "                lemma += \"/\" + tag[:2]\n",
    "                result.append(lemma.encode('utf8'))\n",
    "    res = result\n",
    "    logging.info(\"That's how res looks %s\" %res)\n",
    "    return res\n",
    "    \n",
    "def tokenize(lines, token_size_filter=2):\n",
    "    tokens = fp.mapcat(tokenize_line, lines)\n",
    "    return [t for t in tokens if len(t) > token_size_filter]\n",
    "    \n",
    "\n",
    "def load_doc(filename):\n",
    "    # Slash for linux and double backslash for windows\n",
    "    group, doc_id = filename.split('\\\\')[-2:]\n",
    "    with open(filename) as f:\n",
    "        doc = f.readlines()\n",
    "    logging.info(\"logging in %s in doc %s\" %(group, doc_id))\n",
    "    return {'group': group,\n",
    "            'doc': doc,\n",
    "            'tokens': tokenize(doc),\n",
    "            'id': doc_id}\n",
    "\n",
    "\n",
    "docs = pd.DataFrame(list(map(load_doc, glob('data/20news-bydate-train/*/*')))).set_index(['group','id'])\n",
    "docs.head()\n",
    "# save dataframe to csv file for later usage\n",
    "docs.to_csv(\"data/model/docs.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
