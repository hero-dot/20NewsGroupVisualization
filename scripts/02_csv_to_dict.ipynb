{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from gensim import models\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "import nltk\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get the data from the csv\n",
    "docs = pd.DataFrame.from_csv(\"data/model/docs.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the dictionary, and bag of words corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nltk_stopwords():\n",
    "    return set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def prep_corpus(docs, additional_stopwords=set(), no_below=5, no_above=0.5):\n",
    "    print('Building dictionary...')\n",
    "    dictionary = Dictionary(docs)\n",
    "    # remove stopwords \n",
    "    stopwords = nltk_stopwords().union(additional_stopwords)\n",
    "    stopword_ids = map(dictionary.token2id.get, stopwords)\n",
    "    # get ids for short words len(word)<=3\n",
    "    shortword_ids = [tokenid for tokenid, word in dictionary.iteritems() if len(word.split('/')[0])<= 3]\n",
    "    dictionary.filter_tokens(stopword_ids)\n",
    "    dictionary.compactify()\n",
    "    # get ids for short words len(word)<=3\n",
    "    shortword_ids = [tokenid for tokenid, word in dictionary.iteritems() if len(word.split('/')[0])<= 3]\n",
    "    dictionary.filter_tokens(shortword_ids)\n",
    "    dictionary.compactify()\n",
    "    # remove words that appear only once\n",
    "    once_ids = [tokenid for tokenid, docfreq in dictionary.dfs.iteritems()if docfreq == 1]\n",
    "    dictionary.filter_tokens(once_ids)\n",
    "    dictionary.compactify()\n",
    "    # filter extreme values \n",
    "    dictionary.filter_extremes(no_below=no_below, no_above=no_above, keep_n=None)\n",
    "    dictionary.compactify()\n",
    "\n",
    "    print('Building corpus...')\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "\n",
    "    return dictionary, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dictionary...\n",
      "Building corpus...\n"
     ]
    }
   ],
   "source": [
    "dictionary ,corpus = prep_corpus([literal_eval(doc) for doc in docs['tokens']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MmCorpus.serialize('data/model/newsgroups.mm', corpus)\n",
    "dictionary.save('data/model/newsgroups.dict')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
